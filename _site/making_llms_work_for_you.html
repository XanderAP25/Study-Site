<!DOCTYPE html>

<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" href="/Study-Site/assets/css/styles.css">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Making LLMs Work For You | A Study on LLMs</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Making LLMs Work For You" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/Study-Site/making_llms_work_for_you.html" />
<meta property="og:url" content="http://localhost:4000/Study-Site/making_llms_work_for_you.html" />
<meta property="og:site_name" content="A Study on LLMs" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Making LLMs Work For You" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Making LLMs Work For You","url":"http://localhost:4000/Study-Site/making_llms_work_for_you.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Study-Site/favicon.ico" -->

<!-- end custom head snippets -->


    <!-- MathJax for LaTeX Rendering -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script type="text/javascript" async
      src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
      id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
  </head>

  
  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/XanderAP25/Study-Site">View on GitHub</a>
          

          <h1 id="project_title">Demystifying AI: A Guide to LLMs</h1>

          
          <nav class="site-nav">
  <ul class="nav">
    
      
      <li>
        <a href="/Study-Site/"
          
          >
          Home
        </a>
      </li>
      
    
      
      <li class="dropdown">
        
          <span class="dropdown-label">A Guide to LLMs ▾</span>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/intro_to_ai_and_llms.html"
                
                >
                Introduction to AI and LLMs
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/how_llms_benefit_you_and_their_challenges.html"
                
                >
                How LLMs Benefit You and Their Challenges
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/training_and_fine_tuning_an_llm.html"
                
                >
                Training and Fine-Tuning an LLM
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/making_llms_work_for_you.html"
                
                 class="current" >
                Making LLMs Work for You
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/case_study.html">Case Study ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq"
                 target="_blank" 
                >
                Fine-Tuning a Model on Grimm's Fairy Tales
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq"
                 target="_blank" 
                >
                Generating German Fairy Tales with an LLM
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq"
                 target="_blank" 
                >
                Text Analysis of Output and Grimms
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li>
        <a href="/Study-Site/stories.html"
          
          >
          Stories
        </a>
      </li>
      
    
      
      <li>
        <a href="/Study-Site/about.html"
          
          >
          About
        </a>
      </li>
      
    
  </ul>
</nav>
  
        </header>   
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2 id="making-llms-work-for-you">Making LLMs Work for You</h2>

<p>When it comes to making LLMs work for you, there are two methods of doing so. The first that we will cover is accessing LLMs via APIs.</p>

<p>An LLM API lets you connect your computer directly to ultra powerful AI models, like GPT-4 and Llama 2, without actually having to download and run the model on your computer. What this means for you is that you can have the most mediocre hardware on the planet, but as long as you have a wifi connection and the ability to pay the fee to access the model, you have an LLM that you can use and modify to your heart’s content. The fee to use these APIs is often not a monthly subscription, but instead a fee per million tokens processed as input and output. For reference, one token is about 4 characters, or about three-fourths of a word. This fee depends on what model you’re paying to access, but can be anywhere from \$30 per million tokens processed as output for OpenAI’s GPT-4 Turbo model to \$2.00 per million tokens processed as output for their smaller GPT-3.5 Turbo model. Prices vary depending on what company’s model you use, but there will typically be a price associated with using this larger models.</p>

<p>The cost to use these models presents some concerns when you’re just starting out with using LLMs. For example, what constitutes a heavy workload for these models? You could be getting acquainted with your shiny new LLM and suddenly find yourself forking over way more cash than you initially intended because you didn’t realize that the multi-hour long conversation or learning session you had with your model required millions of input and output tokens to be processed. There are also privacy concerns associated with using LLM APIs, since they typically store your prompts and outputs to improve their own technology. This data is also used to enforce the company’s TOS, prevent illegal activity using their technology, and in specific circumstances can be shared with other parties <sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">1</a></sup>. Those circumstances include: at the request of government authorities, to certain service providers and vendors when business needs must be met, and during the diligence process with those assisting in a business transfer<sup id="fnref:14:1" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">1</a></sup>. Since user data is retained, that also opens you up to the possibility of your data being included in a data leak if the company that provides your LLM gets hacked.</p>

<p>Despite these concerns, LLM APIs are typically at the cutting edge of AI technology. So, if you’re looking for the most powerful model for a specific application, using an API might be the right choice.</p>

<p>As mentioned earlier, another option is downloading and running an LLM directly on your own computer. Running LLMs locally gives you more control over your data, as it is stored entirely on your device and never shared with the model’s creator. There are also no fees for running a local LLM. While this might sound like the ideal option, there are several caveats to consider. For one, these models require fairly powerful hardware to run efficiently. Although there is no fee to use a local LLM, you will likely need to invest in a powerful PC to get the most out of the technology. Smaller models can be run on more modest hardware, such as a relatively modern laptop, but you’ll likely experience slower output and lower quality compared to running top-tier models on a high-performance PC with a strong NVIDIA GPU. (Touch on electricity use and the likelihood of it raising your electricity bill).</p>

<p><strong>(I should also touch on GPT4ALL and LM Studio here, hugging face API free tier)</strong></p>

<p>To get an LLM up and running on your own machine, you will need a platform to run them on. One of the most popular platforms for this purpose is <a href="https://ollama.com">Ollama</a>, an open-source tool that allows you to run AI models that you have downloaded through a terminal. Setup is fairly straightforward, just download the application from the website, and follow the steps it gives you to get up and running. The Ollama website also hosts models for you to get up and running, but you can also run models from Hugging Face if there is something there that is more suited to your needs (i just realized that I have not talked about hugging face at all in this entire article). These models are measured by how many parameters they have, and they are formatted like “8B” meaning 8-billion parameters in this case. For weaker computers, like laptops, you will want to use models that are at most 8B. Depending on how powerful your laptop is, you could go higher than that, especially if it has a dedicated NVIDIA GPU. For PCs, you can go higher than this, but will still find a limit around the 30B parameter mark. Despite that being the limit, there are models available for download with upwards of 70B parameters, going into the 600-billions.</p>

<p>To run these models, you would either need to drop a small fortune on a rig dedicated to running massive AI models, or purchase access to a cloud computing service, and use their hardware instead. In a way, this is the middleground between running an LLM on your own computer and using an API to access a company’s AI model. You have some of the benefits of running an LLM on your machine, like keeping your data private and the ability to pick and choose specific models you want to use on the fly. And some benefits from LLM APIs, like gaining access to an extremely powerful LLM while not needing a powerful computer. You do have to pay to use these cloud services, but these rates are hourly, instead of usage-based, which could be good or bad depending on what you’re doing with your LLM.</p>

<p><strong>(There is a way to properly end this section, but as a first draft, I think this is a good spot to build from after critique)</strong></p>

<p style="text-align:left;">
    <a href="/Study-Site/training_and_fine_tuning_an_llm.html">← Training and Fine-Tuning an LLM</a>
</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:14" role="doc-endnote">
      <p>https://openai.com/policies/row-privacy-policy/ <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:14:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      
      <footer class="inner">
        
        <p class="copyright">Study-Site maintained by <a href="https://github.com/XanderAP25">XanderAP25</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>
  </body>
</html>