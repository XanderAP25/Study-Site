<!DOCTYPE html>

<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" href="/Study-Site/assets/css/styles.css">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Article | A Study on LLMs</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Article" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/Study-Site/article.html" />
<meta property="og:url" content="http://localhost:4000/Study-Site/article.html" />
<meta property="og:site_name" content="A Study on LLMs" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Article" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Article","url":"http://localhost:4000/Study-Site/article.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Study-Site/favicon.ico" -->

<!-- end custom head snippets -->


    <!-- MathJax for LaTeX Rendering -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script type="text/javascript" async
      src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
      id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
  </head>

  
  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/XanderAP25/Study-Site">View on GitHub</a>
          

          <h1 id="project_title">Demystifying AI: A Guide to LLMs</h1>

          
          <nav class="site-nav">
  <ul class="nav">
    
      
      <li>
        <a href="/Study-Site/"
          
          >
          Home
        </a>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/article_landing.html">A Guide to LLMs ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/intro_to_ai_and_llms.html"
                
                >
                Introduction to AI and LLMs
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/how_llms_can_benefit_you_and_their_challenges.html"
                
                >
                How LLMs Can Benefit You and Their Challenges
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/making_llms_work_for_you.html"
                
                >
                Making LLMs Work for You
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/training_and_fine_tuning_an_llm.html"
                
                >
                Training and Fine-Tuning an LLM
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/conclusion.html"
                
                >
                Conclusion
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/case_study.html">Case Study ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="https://colab.research.google.com/drive/1ue50VMGv12nzZ6uQNxL6wITtvgJ0nX5V?usp=sharing"
                 target="_blank" 
                >
                Fine-Tuning a Model on Grimm's Fairy Tales
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1goVTnNt6FauofB_BAQ2Db4h8uWFvfY1d?usp=sharing"
                 target="_blank" 
                >
                Generating German Fairy Tales with an LLM
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq"
                 target="_blank" 
                >
                Text Analysis of Output and Grimms
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/appendix.html">Appendix ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/about.html"
                
                >
                About
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/stories.html"
                
                >
                Stories
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/reading_list.html"
                
                >
                Reading List
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/Acknowledgements.html"
                
                >
                Acknowledgements
              </a>
            </li>
          
        </ul>
      </li>
      
    
  </ul>
</nav>
  
        </header>   
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1 id="demystifying-ai-a-guide-to-llms">Demystifying AI: A Guide to LLMs</h1>

<p>Use this guide as a starting point for your own education into AI, as inspiration for your own projects, or just to expand your existing knowledge. Whether you’re new to the topic or deeply curious about AI’s inner workings, there is something here for you.</p>

<table>
  <thead>
    <tr>
      <th>Table of Contents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="#introduction-to-ai-and-llms">Introduction to AI and LLMs</a></td>
    </tr>
    <tr>
      <td><a href="#how-llms-can-benefit-you-and-their-challenges">How LLMs Can Benefit You and Their Challenges</a></td>
    </tr>
    <tr>
      <td><a href="#making-llms-work-for-you">Making LLMs Work for You</a></td>
    </tr>
    <tr>
      <td><a href="#training-and-fine-tuning-an-llm">Training and Fine-Tuning an LLM</a></td>
    </tr>
    <tr>
      <td><a href="#conclusion">Conclusion</a></td>
    </tr>
  </tbody>
</table>

<h2 id="introduction-to-ai-and-llms">Introduction to AI and LLMs</h2>
<p>Before we explore artificial intelligence, let’s briefly consider what intelligence itself is. Intelligence is granted to us by the feat of evolution that is our brain. So, what can we do with our brains? We can analyze information, learn from experience, reason through problems, and make decisions. Artificial intelligence is the ability of a machine to perform tasks that require human intelligence. There are many models that can fuel artificial intelligence, and we will get into the algorithms and structures behind these models later into the article.</p>

<p>A simple example of a model is linear regression analysis, which works by feeding data into the model as training data, predictors and a target outcome. Based on the training data, the model learns the relationships between the predictors and target variables, allowing it to make predictions.</p>

<p>A more advanced example of a machine learning system is the neural network, which draws inspiration from the neurons in our brains. Like biological neurons, artificial neurons receive input, process it, and pass it along “synapses” to other neurons until an output is eventually produced (<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>). This “input to neuron to output” structure is repeated over and over, sometimes hundreds of thousands of times, just as signals travel through the human brain to form thoughts or trigger physical responses.</p>

<p><img src="assets/images/Neuron Visual (2).png" style="width:650px;display:block; margin:auto;" />
<span style="font-family:Arial; font-size:xx-small; ">Illustration of the traversal of input data through neurons.</span></p>

<p>As shown above, artificial neural networks mimic this process using layers of artificial neurons, or nodes, that process incoming data using mathematical functions. One common function is the sigmoid function (<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>), which normalizes the input value into a range between 0 and 1, allowing the model to determine whether or not to “activate” a connection. Each node in a layer processes its input and passes its result to the next layer, continuing until a final output is produced.</p>

<p>The bulk of this processing happens in the layers between input and output, or the hidden layers, where data is transformed multiple times before a final decision is made (<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>). Whether a node passes its output to another is determined by weights, numerical values that represent the strength of the connection between two nodes. These weights are learned during training. For example, a neural network could be trained to identify whether an image depicts an apple or an orange. During training, the network is fed tons of labeled images of apples and oranges. As it processes the images, the weights between nodes would be adjusted to recognize the features unique to the fruits. Once trained, this network can classify a new input image as either an apple or orange based on the features it observed from the training data.</p>

<p>There are many types of neural networks, each developed to tackle increasingly complex and demanding AI tasks. One notable predecessor to today’s most advanced models is the recurrent neural network (RNN), which was once the standard for handling sequential data (<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). While we won’t cover RNNs in detail here, it’s useful to understand their limitations, especially because they set the stage for the development of a more powerful alternative: the Transformer model. This architecture is the foundation of many modern AI systems, including OpenAI’s ChatGPT.</p>

<p>Before we can get into the Transformer architecture, it’s worth understanding why it replaced RNN. In a standard neural network, information flows in one direction, from input to output, with no memory of previous inputs. This becomes a major obstacle when dealing with sequential data like text, where context is key. RNNs addressed this by introducing loops that allowed the network to maintain a form of memory, but they struggled with long-term dependencies. For example, when processing the sentence:</p>

<p><strong>“I love Rad Cat for his academic due dilligence and intellect.”</strong></p>

<p>The RNN takes each word as input, one at a time, and treats it as largely independent from the words that came before. When it starts with “I,” it has no memory of what’s coming next. If the task were to predict the next word, the network would have to make a guess based solely on “I,” without any understanding of the sentence’s broader context. This is a difficult task even for a human, let alone a machine. To handle this, RNNs introduce a loop in their architecture that allows each output to be fed back in as an input, enabling the model to retain information about earlier words as it moves through the sequence. This gives the network a rudimentary memory, so by the time it reaches “intellect,” it can recall the presence of “academic” earlier in the sentence and use that relationship to inform its output.</p>

<p>The issue with this method, however, lies in its inefficiency and limited memory. RNNs must process sequences step by step, and as the sequence grows longer, their ability to retain earlier information diminishes. For short sentences like the one above, the memory may hold up well enough, but for longer, more complex sequences, earlier words are often “forgotten” as the network progresses (<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). This makes it hard for RNNs to recognize and leverage long-range relationships in text, which are often crucial for understanding nuance, tone, or reference.</p>

<p>The solution to this problem was an entirely new architecture for processing information, the Transformer architecture. The most important concept to understand for the Transformer model is the idea of attention, which is covered at length in <em>Attention is All You Need</em> (<sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>).</p>

<p><img src="assets/images/transformer diagram.png" style="width:300px; height;180px;" /></p>

<p><span style="font-family:Arial; font-size:xx-small; ">Illustration of the Transformer architecture (<sup id="fnref:3:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>).</span></p>

<p>Self-attention is a mechanism that allows a model to look at every word in a sequence at the same time and decide how much attention it should pay to each word in relation to the others (<sup id="fnref:3:4" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). This is incredibly helpful when reading sentences because the model can find relationships between words, even if they are far apart. For example, let’s look at “I love Rad Cat for his academic due dilligence and intellect” again. A self-attention mechanism will process the entire sequence and assign a weight to every pair of words. The mechanism can see that “rad” has a stronger connection to “cat” over “love,” so the weight will be greater and prioritized when making predictions.</p>

<p>At first glance, this might sound similar to how RNNs process sequences. The key difference is the ability for the self-attention mechanism to consider all words of a sequence at once as opposed to sequentially reading through it. This removes the performance bottleneck of processing words one by one, and also addresses the memory limitations of RNNs that cause them to forget relationships between words separated over a long distance.</p>

<p>Now, there is a limitation with self-attention mechanisms. Instead of just running one self-attention mechanism on the model, multi-head attention runs multiple self-attention mechanisms in parallel with each other. Each mechanism, or head, looks at the sequence from a different perspective, and finds different relationships between the words of the sequence. A single self-attention mechanism might only find the relationship between “rad” and “cat” while another could find the relationship between “love” and “intellect.”</p>

<p>This is enabled by the encoding of the words in the input sequence into a numerical representation, called an embedding, which captures the relationships it has with other words in the sequence. This encoding is split up into a number of smaller sections for each head to focus on. At the end of multi-head attention, these relationships are all added back together, leading to a more complete understanding of the input sequence, and a much better ability for predicting the next word in a sequence (<sup id="fnref:3:5" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). At a high-level, that is how Transformer model works. It makes use of multi-head attention to look at a sequence from different perspectives to get a more complete understanding of the relationships between the words than it would have had with just a single attention mechanism, or through RNN. For anyone curious on what the actual implementation of this architecture would look like, there are videos out there on that topic (<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>).</p>

<p>The Transformer model powers modern AI and enables many of the incredible applications we see today. AI is used to translate languages, analyze vast amounts of information to generate summaries, recognize and classify images, create realistic images and videos, and much more. One of the most well-known applications of AI is text generation, where models process a prompt and generate a relevant response. These models, known as Large Language Models (LLMs), specialize in understanding and producing human-like text.  Some of the applications mentioned earlier, like text summarization and language translation also fall under this label. For the rest of this article, we will focus on LLMs, exploring how they work, their benefits, and how you can use them effectively.</p>

<h2 id="how-llms-can-benefit-you-and-their-challenges">How LLMs Can Benefit You and Their Challenges</h2>

<p>At their core, LLMs process text and generate a relevant response. This is a simplified explanation of how LLMs function, but it should be clear what the potential that such a technology holds. Imagine having an assistant that can instantly summarize an article, or a peer to help brainstorm solutions to complex technical problems. What LLMs have to offer is nothing short of revolutionary, but they are not without their downsides. Like any computer program, LLMs require compute power, and lots of it. There are significant environmental concerns that we will look at later in the section that humanity must confront if we continue to use this technology. There are also many difficult ethical questions and concerns that must be addressed, or at the very least acknowledged, to not only inform your use of this technology, but to decide if AI is truly a net good.</p>

<p>For both everyday users and professionals, there is some way in which LLMs can make a tangible impact. Imagine a chatbot that can answer questions you might have on a topic of interest or study, but also help troubleshoot any tech issues you might be facing, or a proofreader that can quickly smooth out your writing. If you are considering a project of any kind, an LLM can work as a sounding board that can help identify angles of approach or ideas that you haven’t yet considered. In the professional space, management consulatants at Boston Consulting Group have been found to complete tasks 25.1% quicker with 40% better quality in their output just by adopting LLMs into their workflow (<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>). Now, imagine applying that to your own workflow—writing reports, coding, or even managing emails. The potential efficiency gains could be substantial. Granted, this is just one case study for a consulting firm, and your personal philosophy on work, or your workflow, may not play as nice with AI. Yet, it still stands as an example of the power of AI when it integrates well with a userbase and workflow.</p>

<p>In education, LLMs are transforming how we teach and learn. Think about the time-consuming tasks teachers face daily: creating assessments, developing lesson plans, providing individualized feedback. LLMs have the potential to handle all of these efficiently. These tools can develop personalized learning materials like summaries, flashcards, and quizzes tailored to individual students (<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>). One study on AI even makes the bold claim that LLMs can make grading more consistent and fair by reducing the subjective biases that naturally creep into human assessment (<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>). There are implications here for the personal role of teachers in education, and how LLMs can potentially hamper it that should be considered before deciding if this is truly a positive change in the education space. For example, LLMs can give individualized feedback, but should they be the ones doing it instead of the human teacher? The AI can learn the quirks, failings, and proficiencies of a student, and tailor its feedback in a robust emulation of a human’s feedback, but is that worth as much as a teacher’s feedback if it is approved by them and saves them time for other teaching duties? Beyond the potential of saving time, LLMs offer powerful language support for students who may not know the native language used in the classroom. What could be most valuable is how using LLMs in classrooms now prepares students for the future AI-driven workforce. As students learn to craft effective prompts through trial and error, they’re building critical thinking skills and technological literacy that will serve them regardless of which careers emerge in our AI-integrated future.</p>

<p>The benefits of LLMs aren’t just limited to any one field or person, they’re already reshaping our world in unexpected ways. For example, in the healthcare, there have been major strides in drug discovery and molecular synthesis with AI models as the key drivers. Insilico is a biotechnology company that has previously used generative AI to develop a drug that is currently in Phase 2 clinical trials (<sup id="fnref:7:1" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>). Although the drug was developed used generative AI rather than an LLM, their new LLM-powered system called Nach0 is capable of designing molecules, predicting their properties, and even suggesting synthesis methods (<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>). While no Nach0 drugs have entered clinical trials yet, there is a possibility that the drugs of the future are designed using AI models that are built upon the LLMs you use today.</p>

<p>While there is a lot of good that LLMs can do for you and the world at large, they are clearly not a miracle technology with zero drawbacks. There is some nuance to their use, and a lot of their benefits are still in the early stages, or not fully realized yet. One of the most controversal downsides to their very existence is how their training data is obtained and used. We’ll be covering this more extensively in the following sections, but LLMs need vast amounts of data to get as competent as they’ve become today. An easy way to get this data is to scrape the entire internet. The owners and creators of digital content are typically not asked for their consent when their material is used to train AI systems, and this has led to tons of debate on the ethical and legal ramifications of training AI. Currently, there is no law against scraping data from the web and using it to train your models, but some organizations like the <em>New York Times</em>, have moved forward with lawsuits against both OpenAI and Microsoft over the use of their copyrighted content to train their models (<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>). This raises an ethical dilemma: should AI companies profit from models trained on content they don’t own? Opponents, like the case <em>The New York Times’s</em> lawsuit, argue that it disregards creators’ rights and exploits their work without compensation. Proponents counter that broad data scraping is essential for AI advancement and the benefits it brings, essentially saying that accurate and higher quality outputs justify the any infringement on people’s intellectual property that may occur.</p>

<p>You might have also wondered how the broad sections of the internet are prepared for AI training. It’s not as simple as dumping raw web pages into an algorithm to train a model. Data needs to be cleaned, filtered, and categorized. Since algorithms aren’t perfect at identifying harmful or low-quality content, much of this work falls on human moderators. These workers, often from lower-income countries, are tasked with reviewing massive amounts of disturbing material containg graphic violence, hate speech, and child exploitation, all to keep AI models “safe” for users. The psychological toll of this work is severe, with many moderators reporting trauma and PTSD-like symptoms from prolonged exposure (<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>). Yet, these workers remain largely invisible, underpaid, and unprotected, raising serious ethical concerns about the unseen labor behind AI advancements. The research paper “On the Dangers of Stochastic Parrots: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency” covers many of these downsides at length, and while I will continue to draw from for covering the downsides of AI, I urge anyone with an interest in AI to read through the 10 page paper (<sup id="fnref:10:1" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>).</p>

<p>Beyond data processing, LLMs also struggle with transparency. Most training datasets are poorly documented, making it difficult to track biases, misinformation, or potential harm (<sup id="fnref:10:2" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>). Even today, many popular models on Hugging Face do not disclose their training data well. This lack of accountability disproportionately affects marginalized groups, as AI models trained on biased data tend to reinforce existing inequalities. From racial and gender biases in generated text to the erasure of underrepresented dialects and cultures, LLMs have the potential to reflect and amplify the flaws of the data they consume (<sup id="fnref:10:3" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>). We’ve seen an example of the impact unchecked training data can have on an AI with the case of the Twitter chatbot Tay, which was designed to learn based on its interactions with users of the website. Within 24 hours the bot was shut down after it started spouting hateful rhetoric that it learned from the website (<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">11</a></sup>).</p>

<p>Compound the potential for bias and hatred with the environmental cost. Training and running large AI models require immense computational power, consuming as much energy as entire cities (<sup id="fnref:10:4" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>). Even simple queries to ChatGPT consume far more energy than what is required for a Google search. As AI development accelerates, so does its carbon footprint, raising concerns about sustainability and the long-term impact on our planet. We were already facing climate challenges prior to the explosion of AI’s popularity, and regardless of the benefits they may bring, the harm they will inflict on our planet is undoubtedly one of the biggest obstacles to widespread adoption and acceptance.</p>

<p>Access to LLMs is also not universal, many people, especially in lower-income regions, are excluded from their benefits due to infrastructure limitations, language barriers, or financial costs (<sup id="fnref:10:5" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>). Yet, these same communities often bear the brunt of AI’s environmental impact, as the energy-intensive data centers powering these models contribute to climate change and resource consumption that disproportionately affect marginalized populations (<sup id="fnref:10:6" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>). This imbalance raises questions about who truly benefits from AI advancements and who is left dealing with the consequences.</p>

<p>AI is a revolutionary technology that can massively improve your workflow, education, and potentially lead to the rapid creation of drugs for diseases that we have not found the cure to yet, but its development has relied on ethically questionable practices, from exploitative labor to environmental harm. Anyone using it extensively should reflect on whether they’re comfortable with these trade-offs. It’s an uncomfortable but necessary conversation—one that, by acknowledging AI’s downsides, can push us toward a future where these issues are meaningfully addressed.</p>

<h2 id="making-llms-work-for-you">Making LLMs Work for You</h2>

<p>When it comes to making LLMs work for you, there are two main methods. The first we will cover is accessing LLMs via APIs.</p>

<p>An LLM API lets you connect your computer directly to ultra-powerful AI models, like GPT-4 and Llama 2, without having to download and run the model on your own machine. What this means for you is that regardless of your hardware, as long as you have a Wi-Fi connection and the ability to pay the fee to access the model, you have an LLM that you can use and modify to your heart’s content. The fee to use these APIs is often not a monthly subscription, but instead a fee per million tokens processed as input and output. This practice has become the standard across most AI companies, but there are cases of some AIs, like Mistral, offering monthly subscriptions. For reference, one token is about four characters, or about three-fourths of a word. This fee depends on what model you’re paying to access but can range from \$30 per million tokens processed as output for OpenAI’s GPT-4 Turbo model to \$2.00 per million tokens processed as output for their smaller GPT-3.5 Turbo model. Prices vary depending on which company’s model you use, but there will typically be a price associated with using these larger models.</p>

<p>The cost to use these models presents some concerns when you’re just starting out using LLMs. For example, what constitutes a heavy workload for these models? You could be getting acquainted with your shiny new LLM and suddenly find yourself paying way more than you initially intended because you didn’t realize that the multi-hour-long conversation or learning session you had with your model required millions of input and output tokens to be processed. There are also privacy concerns associated with using LLM APIs, since they typically store your prompts and outputs to improve their own technology. This data is also used to enforce the company’s TOS, prevent illegal activity using their technology, and, in specific circumstances, can be shared with other parties (<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">12</a></sup>). Those circumstances include: at the request of government authorities, to certain service providers and vendors when business needs must be met, and during the diligence process with those assisting in a business transfer (<sup id="fnref:14:1" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">12</a></sup>). Since user data is retained, that also opens you up to the possibility of your data being included in a data leak if the company that provides your LLM gets hacked.</p>

<p>Despite these concerns, LLM APIs are typically at the cutting edge of AI technology. So, if you’re looking for the most powerful model for a specific application, using an API might be the right choice.</p>

<p>If relying on an external API doesn’t suit your needs or priorities, an alternative approach is to run an LLM directly on your own computer. Running LLMs locally gives you more control over your data, as it is stored entirely on your device and never shared with the model’s creator. There are also no fees for running a local LLM, outside of a possible increase in your electricity bill if you are running your LLMs for extended periods of time. While this might sound like the ideal option, there are several caveats to consider. For one, these models require fairly powerful hardware to run efficiently. Although there is no fee to use a local LLM, you will likely need to invest in a powerful PC to get the most out of the technology. Smaller models can be run on more modest hardware, such as a relatively modern laptop, but you’ll likely experience slower output and lower quality compared to running top-tier models on a high-performance PC with a strong NVIDIA GPU. There are also smaller quantized versions of larger LLMs that offer high quality output, while being able to run on less powerful hardware (<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">13</a></sup>).</p>

<p>To get an LLM up and running on your own machine, you will need a platform to run them on. One of the most popular platforms for this purpose is <a href="https://ollama.com">Ollama</a>, an open-source tool that allows you to run AI models that you have downloaded through a terminal. Setup is fairly straightforward, just download the application from the website and follow the steps it gives you to get up and running. The Ollama website also hosts models for you to use immediately, but you can also run community models from Hugging Face, or custom models of your own design, if there is a specific LLM that is more suited to your needs. Other than Ollama, <a href="https://lmstudio.ai">LM Studio</a> is a similar application that provides a graphical interface to interact with your AI models through instead of a terminal like Ollama. There are also web-based frontends for Ollama that you can get up and running, but they require some technical knowledge that I will not be covering here (<sup id="fnref:20" role="doc-noteref"><a href="#fn:20" class="footnote" rel="footnote">14</a></sup>).</p>

<p>Another popular option for running local models is <a href="https://www.nomic.ai/gpt4all">GPT4All</a>. It stands out for its user-friendliness, offering a clean, ChatGPT-like interface. Unlike platforms like Ollama, which rely on terminal commands, GPT4All is more plug-and-play. Once installed, it allows you to easily browse and download models directly within the application itself, requiring no additional repositories or command-line interactions. GPT4All also kindly lists the RAM requirements for its models, which is incredibly helpful when deciding what model you want to use.</p>

<p>AI models are measured by how many parameters they have, and they are formatted like “8B,” meaning 8 billion parameters in this case. For every day computers, like laptops, you will want to use models that are at most 8B. Depending on how powerful your laptop is, you could go higher than that, especially if it has a dedicated NVIDIA GPU. For desktop PCs with powerful GPUs, you can go higher than this but will still find a limit around the 30B parameter mark. Despite that, there are models available for download with upwards of 70B parameters, going into the hundreds of billions parameters.</p>

<p>To run those massive models, you would either need to drop a small fortune on a rig dedicated to running massive AI models or purchase access to a cloud computing service and use their hardware instead. In a way, this is the middle ground between running an LLM on your own computer and using an API to access a company’s AI model. You have some of the benefits of running an LLM on your machine, like keeping your data private and the ability to pick and choose specific models you want to use on the fly. And some benefits from LLM APIs, like gaining access to an extremely powerful LLM while not needing a powerful computer. You do have to pay to use these cloud services, but these rates are hourly instead of usage-based, which could be good or bad depending on what you’re doing with your LLM.</p>

<p>In my case, where I wanted to not only generate stories, but also fine-tune a model (which we will get into in the next section), I opted to purchase compute units from Google to use with their Google Colab notebook environment. I was granted access to an incredibly powerful GPU that allowed me to generate stories at a far faster rate than I would be able to on my local machine, while also allowing easy sharing of my code due to it being online. For most introductory AI applications, I would recommend using Google Colab, since there is a free tier that would allow you to experiment with more powerful models than you might be able to on your machine.</p>

<p>Ultimately, the choice of interface and machine when using an LLM will be based on your own needs and preferences. If you just want a private chatbot that lacks the usage limits that ChatGPT and the other free tiers of popular AI models have, then Ollama, LM Studio, or GPT4All are likely going to be all you need, assuming you have a machine capable of running smaller or quanitzed models. If you are looking into tuning or training your own LLM or using a top of the line LLM with no compromises, then an API or cloud-computing solution will be more up your alley. Just do your due diligence in finding an API or solution that works for you and fits into your workflow.</p>

<h2 id="training-and-fine-tuning-an-llm">Training and Fine-Tuning an LLM</h2>

<p>We briefly covered how models are trained in the first section, but let’s recap. During training, models process massive sequences of text, analyzing how words relate to one another within a given context. By identifying these relationships, the model learns to predict and generate coherent text based on patterns in its training data. These pre-trained LLMs are what most people will be working with when setting up their own specialized LLM, largely due to the fact that not everyone has the ability to dedicate unfathomable amounts of energy towards the LLM of their dreams. This is where in-context learning (ICL) and fine-tuning come into play.</p>

<p>Both of these techniques influence the LLM’s output, but the way each of them work is fundamentally different. Fine-tuning involves retraining the model itself on new data, altering its parameters. In-Context Learning does not require any tweaks to the parameters for the model to become specialized to whatever task you are training it for. Instead, you provide the model with examples, and it learns on the fly as you attempt to give it an education on whatever topic you are trying to specialize it for (<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">15</a></sup>). This is really useful if you want to have a model specialized for tasks such as text classification, sentiment analysis, or language translation, since you can give a bunch of examples to the AI, and have it learn how to do these tasks for you. Let’s take sentiment analysis as an example. You want to have your LLM know how to tell whether a review on a movie is positive or negative. To do this, you want to provide a set of demonstration examples for it to follow through a prompt.</p>

<table>
  <thead>
    <tr>
      <th>Prompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Answer the upcoming prompts using this format: <br />Review: This movie was nothing but a waste of my time and money, I’m done with this franchise. Sentiment: Negative<br />Review: The ultimate conclusion to a story five years in the making. I am speechless. Sentiment: Positive<br />Review: While the visuals and flashy fights were enough to get me through the movie, I just could not engage with the story. Sentiment: Negative</td>
    </tr>
  </tbody>
</table>

<p>That is all you need to do for a basic implementation of ICL, just give the LLM an example to follow, and it will adhere to it to the best of its abilities. Now, there are some complexities to this technique that can let you get the most out of it. Specifically, one must consider the selection of what data is used in the prompt, the format that it is presented in, and the order that it is given to the AI (<sup id="fnref:11:1" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">15</a></sup>). We want to provide relevant examples that are formatted in the most consumable format for the AI, and these examples should build on each other logically. Taking time to work out how you’ll educate your AI before actually carrying it out will lead to a better result. For more information on the techniques involved in maximizing In-Context Learning, I would recommend checking out <em>A Survey on In-Context Learning</em> and the numerous sources it points to for improved learning practices (<sup id="fnref:11:2" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">15</a></sup>).</p>

<p>You can see how I used ICL to improve an LLM’s story generation capabilities in my story generation code notebook (<sup id="fnref:15:1" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">11</a></sup>).</p>

<p>Fine-tuning is more involved than ICL, requiring modification of the model’s parameters by training it on new data that is specialized for the intended function that you want the LLM to perform. Since the LLM is being retrained, the end result is a model that can more effectively carry out the task given to it. The process of fine-tuning requires a relevant dataset. If that is a sentiment analysis model, then the input dataset could look like this:</p>

<table>
  <thead>
    <tr>
      <th>Text</th>
      <th>Sentiments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>This movie was nothing but a waste of my time and money, I’m done with this franchise.</td>
      <td>Negative</td>
    </tr>
    <tr>
      <td>The ultimate conclusion to a story five years in the making. I am speechless.</td>
      <td>Positive</td>
    </tr>
    <tr>
      <td>While the visuals and flashy fights were enough to get me through the movie, I just could not engage with the story.</td>
      <td>Negative</td>
    </tr>
  </tbody>
</table>

<p>In a real-world application, you would want a lot more than just three reviews, but this should illustrate the type of data that is fed to models for fine-tuning.</p>

<p>From here, the model’s parameters are tweaked on the dataset, meaning that its understanding of movie review sentiment is being ingrained directly into the model. The key benefit of fine-tuning over ICL is that fine-tuning creates a more permanent integration of the new information into the LLM’s structure. Just like a person, LLMs “forget” what they learned after a session ends. Instead of having to feed an LLM the same prompt every time you want to have it learn how to do a specific task, you can finetune a model to permanently remember how to do said task.</p>

<p>You can see how I used a dataset consisting of German Fairy Tales to fine-tune an LLM in my fine-tuning code notebook (<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">16</a></sup>). If you are interested in seeing how the fine-tuned model compares to a baseline model using ICL, I recommend taking a look at my text analysis code notebook (<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">17</a></sup>). If not, then just know that fine-tuning a model in this case led to generated stories that were close in structure and themes to actual German Fairy Tales than a model using ICL, but it came at significantly higher costs to efficiency. The stories generated using ICL prompts were also not too far behind the fine-tuned model’s stories in closeness to the actual fairy tales, meaning that depending on your needs, ICL may be preferred over fine-tuning despite the loss in quality. The process of fine-tuning is quite costly, and while I touch on it in my case study notebooks, I recommend you take a look at the footnote below for more information (<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">18</a></sup>).</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this guide, we explored the Transformer architecture and highlighted how attention mechanisms contribute to the powerful text processing capabilities seen in LLMs. These models offer immense benefits, from enhancing workflows to providing innovative solutions across various fields. However, alongside these advantages come significant environmental, ethical, legal, and moral considerations that must be acknowledged. It’s crucial for anyone looking to harness the potential of AI to understand these challenges and use the technology responsibly.</p>

<p>To make effective use of this technology, it’s important to carefully consider which API, cloud service, or application best suits your use case, while also determining whether in-context learning or fine-tuning is the best approach to tailoring a model to your needs.</p>

<p>With that said, I hope you found something useful from this guide, and I wish you luck in your own AI journey. If you haven’t already, I highly recommend checking out my case studies. The code notebooks are a great introduction into how one might use LLMs for their own interests, from fine-tuning a model, to crafting ICL prompts, to generating desired output.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Han, Su-Hyun, et al. “Artificial Neural Network: Understanding the Basic Concepts without Mathematics.” <em>Https://Doi.Org/10.12779/Dnd.2018.17.3.83</em>, 17 Sept. 2018, doi.org/10.12779/dnd.2018.17.3.83. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>$\sigma(x) = \frac{1}{1+e^{-x}}$ where $x$ is the sum of all the inputs going into the node. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Vaswani, Ashish, et al. “Attention Is All You Need.” arXiv.Org, 2 Aug. 2023, arxiv.org/abs/1706.03762. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:3:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:3:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:3:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>A video walkthrough of the implementation of the Transformer model: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Dell’Acqua, Fabrizio, et al. “Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.” SSRN, 18 Sept. 2023, papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Javaid, Muhammad, et al. “Unlocking the Opportunities through CHATGPT Tool towards Ameliorating the Education System.” BenchCouncil Transactions on Benchmarks, Standards and Evaluations, no. 2, 2023, p. 100115. https://doi.org/10.1016/j.tbench.2023.100115. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Alqahtani, Tariq, et al. “The Emergent Role of Artificial Intelligence, Natural Learning Processing, and Large Language Models in Higher Education and Research.” Research in Social and Administrative Pharmacy, 2023. https://doi.org/10.1016/j.sapharm.2023.05.016. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:7:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Yao, Renee. “Quicker Cures: How Insilico Medicine Uses Generative AI to Accelerate Drug Discovery.” NVIDIA Blog, 16 Oct. 2024, blogs.nvidia.com/blog/insilico-medicine-uses-generative-ai-to-accelerate-drug-discovery/. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Michael. “The Times Sues OpenAI and Microsoft over A.I. Use of Copyrighted Work.” The New York Times, The New York Times, 27 Dec. 2023, www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Bender, Emily M., et al. “On the Dangers of Stochastic Parrots: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.” ACM Conferences, 1 Mar. 2021, <em>dl.acm.org/doi/10.1145/3442188.3445922.</em> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:10:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:10:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:10:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:10:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:10:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a> <a href="#fnref:10:6" class="reversefootnote" role="doc-backlink">&#8617;<sup>7</sup></a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p><a href="https://colab.research.google.com/drive/1goVTnNt6FauofB_BAQ2Db4h8uWFvfY1d" target="_blank">Story Generation Using an LLM in a Notebook Environment</a> <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:15:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>https://openai.com/policies/row-privacy-policy/ <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:14:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p>Quantized models have their weights reduced in size from (typically) 32-bit floats to 8-bit integers. This significantly reduces their size and computational overhead while making them less precise in their outputs, meaning that they will have somewhat lower quality generated text. These models still benefit from large training sets and expert optimization, however. For further reading, see Hugging Face’s documentation on quantization: https://huggingface.co/docs/optimum/en/concept_guides/quantization <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:20" role="doc-endnote">
      <p>Any web-based frontend for Ollama will require <a href="https://docs.docker.com/get-started/">Docker</a>  to be installed onto your machine. I recommend having some knowledge of Docker if you’re at all interested in software development, so this could be a good introduction to that tool. This front-end is a popular choice for users of Ollama: https://github.com/open-webui/open-webui <a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>Dong, Qingxiu, et al. “A Survey on In-Context Learning.” arXiv.Org, 5 Oct. 2024, arxiv.org/abs/2301.00234. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:11:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:11:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:16" role="doc-endnote">
      <p><a href="https://colab.research.google.com/drive/1ue50VMGv12nzZ6uQNxL6wITtvgJ0nX5V?usp=sharing" target="_blank">Finetuning an LLM on German Fairy Tales</a> <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p><a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq" target="_blank">Text Analysis of Output and Grimms</a> <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:18" role="doc-endnote">
      <p>LLMs use tons of VRAM, and require powerful GPUs to be used to their fullest extent. Whether that be output generation, or fine-tuning, you want to have a lot of VRAM at your disposal. You can get away with using CPUs or Apple Silicon in conjunction with your system RAM for running and fine-tuning smaller LLMs, but you will see noticeably less performance and efficiency. Google Colab is a relatively cheap resource to rent powerful GPUs that you can easily use for your AI ambitions. You can even use a free T4 GPU, which offers more power than what the layman would likely have on hand. A good rule of thumb is that you want at least 16 GB of VRAM to do anything significant with moderate sized LLMs (1-3B parameters), and you may want 80GB+ of VRAM if you’re fine-tuning larger models. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      
      <footer class="inner">
        
        <p class="copyright">Study-Site maintained by <a href="https://github.com/XanderAP25">XanderAP25</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>
  </body>
</html>