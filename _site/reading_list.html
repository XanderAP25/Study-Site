<!DOCTYPE html>

<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" href="/Study-Site/assets/css/styles.css">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reading List | A Study on LLMs</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Reading List" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/Study-Site/reading_list.html" />
<meta property="og:url" content="http://localhost:4000/Study-Site/reading_list.html" />
<meta property="og:site_name" content="A Study on LLMs" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reading List" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Reading List","url":"http://localhost:4000/Study-Site/reading_list.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Study-Site/favicon.ico" -->

<!-- end custom head snippets -->


    <!-- MathJax for LaTeX Rendering -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script type="text/javascript" async
      src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
      id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
  </head>

  
  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/XanderAP25/Study-Site">View on GitHub</a>
          

          <h1 id="project_title">Demystifying AI: A Guide to LLMs</h1>

          
          <nav class="site-nav">
  <ul class="nav">
    
      
      <li>
        <a href="/Study-Site/"
          
          >
          Home
        </a>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/article_landing.html">A Guide to LLMs ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/intro_to_ai_and_llms.html"
                
                >
                Introduction to AI and LLMs
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/how_llms_can_benefit_you_and_their_challenges.html"
                
                >
                How LLMs Can Benefit You and Their Challenges
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/making_llms_work_for_you.html"
                
                >
                Making LLMs Work for You
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/training_and_fine_tuning_an_llm.html"
                
                >
                Training and Fine-Tuning an LLM
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/conclusion.html"
                
                >
                Conclusion
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/case_study.html">Case Study ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="https://colab.research.google.com/drive/1ue50VMGv12nzZ6uQNxL6wITtvgJ0nX5V?usp=sharing"
                 target="_blank" 
                >
                Fine-Tuning a Model on Grimm's Fairy Tales
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1goVTnNt6FauofB_BAQ2Db4h8uWFvfY1d?usp=sharing"
                 target="_blank" 
                >
                Generating German Fairy Tales with an LLM
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq"
                 target="_blank" 
                >
                Text Analysis of Output and Grimms
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/appendix.html">Appendix ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/about.html"
                
                >
                About
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/stories.html"
                
                >
                Stories
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/reading_list.html"
                
                 class="current" >
                Reading List
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/Acknowledgements.html"
                
                >
                Acknowledgements
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/contact_information.html"
                
                >
                Contact Information
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/usage_rights.html"
                
                >
                Usage Rights
              </a>
            </li>
          
        </ul>
      </li>
      
    
  </ul>
</nav>
  
        </header>   
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1 id="reading-list">Reading List</h1>

<p>The following sources informed the theoretical foundation and analysis presented in both <em>Demystifying AI: A Guide to LLMs</em> and the accompanying case study.</p>

<h3 id="how-aillms-work">How AI/LLMs Work</h3>

<ul>
  <li>
    <p><strong>Bender, Emily M., et al.</strong> “On the Dangers of Stochastic Parrots: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.” <em>ACM Conferences</em>, 1 Mar. 2021. <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">dl.acm.org/doi/10.1145/3442188.3445922</a><br />
<em>Highlights the dangers of LLMs, emphasizing that they link words based on probability without true understanding. Central for framing ethical and conceptual discussions in this study.</em></p>
  </li>
  <li>
    <p><strong>Kojima, Takeshi, et al.</strong> “Large Language Models Are Zero-Shot Reasoners.” <em>arXiv.Org</em>, 29 Jan. 2023. <a href="https://arxiv.org/abs/2205.11916">arxiv.org/abs/2205.11916</a><br />
<em>Introduces Zero-Shot Learning—where AI models can generalize to unseen tasks. Useful to understand before training a model on German fairy tales.</em></p>
  </li>
  <li>
    <p><strong>Sui, Peiqi, et al.</strong> “Confabulation: The Surprising Value of Large Language Model Hallucinations.” <em>arXiv.Org</em>, 25 June 2024. <a href="https://arxiv.org/abs/2406.04175">arxiv.org/abs/2406.04175</a><br />
<em>Examines hallucinations in LLMs and argues that they may have value in benchmarking narrativity and coherence in generated outputs. Relevant for evaluating story generation.</em></p>
  </li>
  <li>
    <p><strong>Vaswani, Ashish, et al.</strong> “Attention Is All You Need.” <em>arXiv.Org</em>, 2 Aug. 2023. <a href="https://arxiv.org/abs/1706.03762">arxiv.org/abs/1706.03762</a><br />
<em>Foundational paper introducing the Transformer architecture that underpins modern LLMs. This forms the theoretical basis for much of the study.</em></p>
  </li>
  <li>
    <p><strong>Walsh, Melanie, et al.</strong> “Does ChatGPT Have a Poetic Style?” <em>arXiv.Org</em>, 30 Oct. 2024. <a href="https://arxiv.org/abs/2410.15299">arxiv.org/abs/2410.15299</a><br />
<em>Analyzes poetry generated by LLMs, identifying stylistic uniformity compared to human poetry. Useful reference for text analysis and stylistic evaluation in the study.</em></p>
  </li>
</ul>

<h3 id="how-to-do-ai">How to Do AI</h3>

<ul>
  <li>
    <p><strong>Dong, Qingxiu, et al.</strong> “A Survey on In-Context Learning.” <em>arXiv.Org</em>, 5 Oct. 2024. <a href="https://arxiv.org/abs/2301.00234">arxiv.org/abs/2301.00234</a><br />
<em>Comprehensive overview of in-context learning techniques, challenges, and applications. Core resource for understanding methods used in training and prompting models.</em></p>
  </li>
  <li>
    <p><strong>Meng, Kevin, et al.</strong> “Mass-Editing Memory in a Transformer.” <em>arXiv.Org</em>, 1 Aug. 2023. <a href="https://arxiv.org/abs/2210.07229">arxiv.org/abs/2210.07229</a><br />
<em>Introduces MEMIT, a fine-tuning technique for efficiently updating many memories in LLMs. Relevant for fine-tuning work on German fairy tales.</em></p>
  </li>
  <li>
    <p><strong>Min, Sewon, et al.</strong> “MetaICL: Learning to Learn in Context.” <em>arXiv.Org</em>, 3 May 2022. <a href="https://arxiv.org/abs/2110.15943">arxiv.org/abs/2110.15943</a><br />
<em>Presents a meta-training framework for improving in-context learning performance. Helpful in exploring methods to enhance model task specialization.</em></p>
  </li>
  <li>
    <p><strong>Padmanabhan, Shankar, et al.</strong> “Propagating Knowledge Updates to LMs through Distillation.” <em>arXiv.Org</em>, 31 Oct. 2023. <a href="https://arxiv.org/abs/2306.09306">arxiv.org/abs/2306.09306</a><br />
<em>Proposes context distillation as an alternative to fine-tuning for knowledge editing. Useful for understanding different model updating strategies.</em></p>
  </li>
</ul>

<h3 id="what-ai-is-used-for">What AI Is Used For</h3>

<ul>
  <li>
    <p><strong>Beguš, Nina.</strong> “Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling.” <em>Nature</em>, 28 Oct. 2024. <a href="https://www.nature.com/articles/s41599-024-03868-8">nature.com/articles/s41599-024-03868-8</a><br />
<em>Compares storytelling from GPT models and human authors under identical prompts. Provides a benchmark for evaluating generated stories.</em></p>
  </li>
  <li>
    <p><strong>Hicke, Rebecca M. M., et al.</strong> “Says Who? Effective Zero-Shot Annotation of Focalization.” <em>arXiv.Org</em>, 17 Sept. 2024. <a href="https://arxiv.org/abs/2409.11390">arxiv.org/abs/2409.11390</a><br />
<em>Demonstrates using LLMs to annotate literary texts for focalization. Relevant for literary and text analysis of generated stories.</em></p>
  </li>
  <li>
    <p><strong>Sakellaridis, Pavlos.</strong> “Exploring the Potential of LLM-Based Agents as Dungeon Masters.” <em>Studednttheses.Uu.Nl</em>, July 2024. <a href="https://studenttheses.uu.nl/bitstream/handle/20.500.12932/47209/Thesis_Final.pdf?sequence=1&amp;isAllowed=y">studenttheses.uu.nl/handle/20.500.12932/47209</a><br />
<em>Examines the use of LLMs as Dungeon Masters in D&amp;D, trained on Critical Role datasets. Provides inspiration for interactive narrative applications.</em></p>
  </li>
  <li>
    <p><strong>Walsh, Melanie, Anna Preus, and Maria Antoniak.</strong> “Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets.” <em>arXiv.Org</em>, 10 Oct. 2024. <a href="https://arxiv.org/abs/2406.18906">arxiv.org/abs/2406.18906</a><br />
<em>Explores benchmarking LLM-generated poetry against human forms. Useful model for benchmarking AI-generated fairy tales in the case study.</em></p>
  </li>
</ul>

<h3 id="general-resources">General Resources</h3>

<ul>
  <li>
    <p><strong>The AI for Humanists Project.</strong> <em>AI for Humanists.</em> <a href="https://aiforhumanists.com">aiforhumanists.com</a><br />
<em>A collection of tutorials and resources designed to introduce humanities scholars to text classification, word embeddings, and LLM applications. Includes a notebook on zero-shot prompting and an extensive glossary of key AI terms. Useful as a reference throughout this study for concepts, methods, and practical techniques. Can also serve as a great next step for your own AI journey.</em></p>
  </li>
  <li>
    <p><strong>Willson, Simon.</strong> “Things We Learned about LLMs in 2024.” <em>simonwillison.net</em>, 31 Dec. 2024. <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#some-of-those-gpt-4-models-run-on-my-laptop">simonwillison.net/2024/Dec/31/llms-in-2024</a><br />
<em>A collection of blog posts on emerging LLM trends and practical applications. Offers technical insights and project inspiration.</em></p>
  </li>
</ul>


      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      
      <footer class="inner">
        
        <p class="copyright">Study-Site maintained by <a href="https://github.com/XanderAP25">XanderAP25</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>
  </body>
</html>