<!DOCTYPE html>

<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" href="/Study-Site/assets/css/styles.css">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Introduction to AI and LLMs | A Study on LLMs</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Introduction to AI and LLMs" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/Study-Site/intro_to_ai_and_llms.html" />
<meta property="og:url" content="http://localhost:4000/Study-Site/intro_to_ai_and_llms.html" />
<meta property="og:site_name" content="A Study on LLMs" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Introduction to AI and LLMs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Introduction to AI and LLMs","url":"http://localhost:4000/Study-Site/intro_to_ai_and_llms.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Study-Site/favicon.ico" -->

<!-- end custom head snippets -->


    <!-- MathJax for LaTeX Rendering -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script type="text/javascript" async
      src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script type="text/javascript" async
      id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
  </head>

  
  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/XanderAP25/Study-Site">View on GitHub</a>
          

          <h1 id="project_title">Demystifying AI: A Guide to LLMs</h1>

          
          <nav class="site-nav">
  <ul class="nav">
    
      
      <li>
        <a href="/Study-Site/"
          
          >
          Home
        </a>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/article_landing.html">A Guide to LLMs ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/intro_to_ai_and_llms.html"
                
                 class="current" >
                Introduction to AI and LLMs
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/how_llms_benefit_you_and_their_challenges.html"
                
                >
                How LLMs Benefit You and Their Challenges
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/training_and_fine_tuning_an_llm.html"
                
                >
                Training and Fine-Tuning an LLM
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/making_llms_work_for_you.html"
                
                >
                Making LLMs Work for You
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/case_study.html">Case Study ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="https://colab.research.google.com/drive/1ue50VMGv12nzZ6uQNxL6wITtvgJ0nX5V?usp=sharing"
                 target="_blank" 
                >
                Fine-Tuning a Model on Grimm's Fairy Tales
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1goVTnNt6FauofB_BAQ2Db4h8uWFvfY1d?usp=sharing"
                 target="_blank" 
                >
                Generating German Fairy Tales with an LLM
              </a>
            </li>
          
            <li>
              <a href="https://colab.research.google.com/drive/1_q8NFdDmb1_QonBbXd9wk84RiPA8ei8l?usp=sharing#scrollTo=BMzdSSBlq8dq"
                 target="_blank" 
                >
                Text Analysis of Output and Grimms
              </a>
            </li>
          
        </ul>
      </li>
      
    
      
      <li class="dropdown">
        
          <a href="/Study-Site/appendix.html">Appendix ▾</a>
        
        <ul class="dropdown-menu">
          
            <li>
              <a href="/Study-Site/about.html"
                
                >
                About
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/stories.html"
                
                >
                Stories
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/works_cited.html"
                
                >
                Works Cited
              </a>
            </li>
          
            <li>
              <a href="/Study-Site/Acknowledgements.html"
                
                >
                Acknowledgements
              </a>
            </li>
          
        </ul>
      </li>
      
    
  </ul>
</nav>
  
        </header>   
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2 id="section-one-introduction-to-ai-and-llms">Section One: Introduction to AI and LLMs</h2>

<p>Before we explore artificial intelligence, let’s briefly consider what intelligence itself is. Intelligence is granted to us by the feat of evolution that is our brain. So, what can we do with our brains? We can analyze information, learn from experience, reason through problems, and make decisions. Artificial intelligence is the ability of a machine to perform tasks that require human intelligence. There are many models that can fuel artificial intelligence, and we will get into the algorithms and structures behind these models later into the article.</p>

<p>A simple example of a model is linear regression analysis, which works by feeding data into the model as training data, predictors and a target outcome. Based on the training data, the model learns the relationships between the predictors and target variables, allowing it to make predictions.</p>

<p>A more advanced example of a machine learning system is the neural network, which draws inspiration from the neurons in our brains. Like biological neurons, artificial neurons receive input, process it, and pass it along “synapses” to other neurons until an output is eventually produced (<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>). This “input to neuron to output” structure is repeated over and over, sometimes hundreds of thousands of times, just as signals travel through the human brain to form thoughts or trigger physical responses.</p>

<p><img src="assets/images/Neuron Visual (2).png" style="width:650px;display:block; margin:auto;" />
<span style="font-family:Arial; font-size:xx-small; ">Illustration of the traversal of input data through neurons.</span></p>

<p>As shown above, artificial neural networks mimic this process using layers of artificial neurons, or nodes, that process incoming data using mathematical functions. One common function is the sigmoid function (<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>), which normalizes the input value into a range between 0 and 1, allowing the model to determine whether or not to “activate” a connection. Each node in a layer processes its input and passes its result to the next layer, continuing until a final output is produced.</p>

<p>The bulk of this processing happens in the layers between input and output, or the hidden layers, where data is transformed multiple times before a final decision is made (<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>). Whether a node passes its output to another is determined by weights, numerical values that represent the strength of the connection between two nodes. These weights are learned during training. For example, a neural network could be trained to identify whether an image depicts an apple or an orange. During training, the network is fed tons of labeled images of apples and oranges. As it processes the images, the weights between nodes would be adjusted to recognize the features unique to the fruits. Once trained, this network can classify a new input image as either an apple or orange based on the features it observed from the training data.</p>

<p>There are many types of neural networks, each developed to tackle increasingly complex and demanding AI tasks. One notable predecessor to today’s most advanced models is the recurrent neural network (RNN), which was once the standard for handling sequential data (<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). While we won’t cover RNNs in detail here, it’s useful to understand their limitations, especially because they set the stage for the development of a more powerful alternative: the Transformer model. This architecture is the foundation of many modern AI systems, including OpenAI’s ChatGPT.</p>

<p>Before we can get into the Transformer architecture, it’s worth understanding why it replaced RNN. In a standard neural network, information flows in one direction, from input to output, with no memory of previous inputs. This becomes a major obstacle when dealing with sequential data like text, where context is key. RNNs addressed this by introducing loops that allowed the network to maintain a form of memory, but they struggled with long-term dependencies. For example, when processing the sentence:</p>

<p><strong>“I love Rad Cat for his academic due dilligence and intellect.”</strong></p>

<p>The RNN takes each word as input, one at a time, and treats it as largely independent from the words that came before. When it starts with “I,” it has no memory of what’s coming next. If the task were to predict the next word, the network would have to make a guess based solely on “I,” without any understanding of the sentence’s broader context. This is a difficult task even for a human, let alone a machine. To handle this, RNNs introduce a loop in their architecture that allows each output to be fed back in as an input, enabling the model to retain information about earlier words as it moves through the sequence. This gives the network a rudimentary memory, so by the time it reaches “intellect,” it can recall the presence of “academic” earlier in the sentence and use that relationship to inform its output.</p>

<p>The issue with this method, however, lies in its inefficiency and limited memory. RNNs must process sequences step by step, and as the sequence grows longer, their ability to retain earlier information diminishes. For short sentences like the one above, the memory may hold up well enough, but for longer, more complex sequences, earlier words are often “forgotten” as the network progresses (<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). This makes it hard for RNNs to recognize and leverage long-range relationships in text, which are often crucial for understanding nuance, tone, or reference.</p>

<p>The solution to this problem was an entirely new architecture for processing information, the Transformer architecture. The most important concept to understand for the Transformer model is the idea of attention, which is covered at length in <em>Attention is All You Need</em> (<sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>).</p>

<p><img src="assets/images/transformer diagram.png" style="width:300px; height;180px;" /></p>

<p><span style="font-family:Arial; font-size:xx-small; ">Illustration of the Transformer architecture (<sup id="fnref:3:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>).</span></p>

<p>Self-attention is a mechanism that allows a model to look at every word in a sequence at the same time and decide how much attention it should pay to each word in relation to the others (<sup id="fnref:3:4" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). This is incredibly helpful when reading sentences because the model can find relationships between words, even if they are far apart. For example, let’s look at “I love Rad Cat for his academic due dilligence and intellect” again. A self-attention mechanism will process the entire sequence and assign a weight to every pair of words. The mechanism can see that “rad” has a stronger connection to “cat” over “love,” so the weight will be greater and prioritized when making predictions.</p>

<p>At first glance, this might sound similar to how RNNs process sequences. The key difference is the ability for the self-attention mechanism to consider all words of a sequence at once as opposed to sequentially reading through it. This removes the performance bottleneck of processing words one by one, and also addresses the memory limitations of RNNs that cause them to forget relationships between words separated over a long distance.</p>

<p>Now, there is a limitation with self-attention mechanisms. Instead of just running one self-attention mechanism on the model, multi-head attention runs multiple self-attention mechanisms in parallel with each other. Each mechanism, or head, looks at the sequence from a different perspective, and finds different relationships between the words of the sequence. A single self-attention mechanism might only find the relationship between “rad” and “cat” while another could find the relationship between “love” and “intellect.”</p>

<p>This is enabled by the encoding of the words in the input sequence into a numerical representation, called an embedding, which captures the relationships it has with other words in the sequence. This encoding is split up into a number of smaller sections for each head to focus on. At the end of multi-head attention, these relationships are all added back together, leading to a more complete understanding of the input sequence, and a much better ability for predicting the next word in a sequence (<sup id="fnref:3:5" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>). At a high-level, that is how Transformer model works. It makes use of multi-head attention to look at a sequence from different perspectives to get a more complete understanding of the relationships between the words than it would have had with just a single attention mechanism, or through RNN. For anyone curious on what the actual implementation of this architecture would look like, there are videos out there on that topic (<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>).</p>

<p>The Transformer model powers modern AI and enables many of the incredible applications we see today. AI is used to translate languages, analyze vast amounts of information to generate summaries, recognize and classify images, create realistic images and videos, and much more. One of the most well-known applications of AI is text generation, where models process a prompt and generate a relevant response. These models, known as Large Language Models (LLMs), specialize in understanding and producing human-like text.  Some of the applications mentioned earlier, like text summarization and language translation also fall under this label. For the rest of this article, we will focus on LLMs, exploring how they work, their benefits, and how you can use them effectively.</p>

<p style="text-align:right;">
    <a href="/Study-Site/how_llms_benefit_you_and_their_challenges.html" style="padding: 0.4em 0.8em; border: 1px solid #1e6bb8; color: #1e6bb8; text-decoration: none; border-radius: 3px; font-weight: bold;">How LLMs Benefit You and Their Challenges →</a>
</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Han, Su-Hyun, et al. “Artificial Neural Network: Understanding the Basic Concepts without Mathematics.” <em>Https://Doi.Org/10.12779/Dnd.2018.17.3.83</em>, 17 Sept. 2018, doi.org/10.12779/dnd.2018.17.3.83. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>$\sigma(x) = \frac{1}{1+e^{-x}}$ where $x$ is the sum of all the inputs going into the node. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Vaswani, Ashish, et al. “Attention Is All You Need.” arXiv.Org, 2 Aug. 2023, arxiv.org/abs/1706.03762. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:3:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:3:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:3:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>A video walkthrough of the implementation of the Transformer model: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      
      <footer class="inner">
        
        <p class="copyright">Study-Site maintained by <a href="https://github.com/XanderAP25">XanderAP25</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>
  </body>
</html>