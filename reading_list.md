---
layout: default
title: Reading List
---

# Reading List

The following sources informed the theoretical foundation and analysis presented in both *Demystifying AI: A Guide to LLMs* and the accompanying case study.

### How AI/LLMs Work

- **Bender, Emily M., et al.** “On the Dangers of Stochastic Parrots: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.” *ACM Conferences*, 1 Mar. 2021. [dl.acm.org/doi/10.1145/3442188.3445922](https://dl.acm.org/doi/10.1145/3442188.3445922)  
  *Highlights the dangers of LLMs, emphasizing that they link words based on probability without true understanding. Central for framing ethical and conceptual discussions in this study.*

- **Kojima, Takeshi, et al.** “Large Language Models Are Zero-Shot Reasoners.” *arXiv.Org*, 29 Jan. 2023. [arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)  
  *Introduces Zero-Shot Learning—where AI models can generalize to unseen tasks. Useful to understand before training a model on German fairy tales.*

- **Sui, Peiqi, et al.** “Confabulation: The Surprising Value of Large Language Model Hallucinations.” *arXiv.Org*, 25 June 2024. [arxiv.org/abs/2406.04175](https://arxiv.org/abs/2406.04175)  
  *Examines hallucinations in LLMs and argues that they may have value in benchmarking narrativity and coherence in generated outputs. Relevant for evaluating story generation.*

- **Vaswani, Ashish, et al.** “Attention Is All You Need.” *arXiv.Org*, 2 Aug. 2023. [arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  
  *Foundational paper introducing the Transformer architecture that underpins modern LLMs. This forms the theoretical basis for much of the study.*

- **Walsh, Melanie, et al.** “Does ChatGPT Have a Poetic Style?” *arXiv.Org*, 30 Oct. 2024. [arxiv.org/abs/2410.15299](https://arxiv.org/abs/2410.15299)  
  *Analyzes poetry generated by LLMs, identifying stylistic uniformity compared to human poetry. Useful reference for text analysis and stylistic evaluation in the study.*

### How to Do AI

- **Dong, Qingxiu, et al.** “A Survey on In-Context Learning.” *arXiv.Org*, 5 Oct. 2024. [arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234)  
  *Comprehensive overview of in-context learning techniques, challenges, and applications. Core resource for understanding methods used in training and prompting models.*

- **Meng, Kevin, et al.** “Mass-Editing Memory in a Transformer.” *arXiv.Org*, 1 Aug. 2023. [arxiv.org/abs/2210.07229](https://arxiv.org/abs/2210.07229)  
  *Introduces MEMIT, a fine-tuning technique for efficiently updating many memories in LLMs. Relevant for fine-tuning work on German fairy tales.*

- **Min, Sewon, et al.** “MetaICL: Learning to Learn in Context.” *arXiv.Org*, 3 May 2022. [arxiv.org/abs/2110.15943](https://arxiv.org/abs/2110.15943)  
  *Presents a meta-training framework for improving in-context learning performance. Helpful in exploring methods to enhance model task specialization.*

- **Padmanabhan, Shankar, et al.** “Propagating Knowledge Updates to LMs through Distillation.” *arXiv.Org*, 31 Oct. 2023. [arxiv.org/abs/2306.09306](https://arxiv.org/abs/2306.09306)  
  *Proposes context distillation as an alternative to fine-tuning for knowledge editing. Useful for understanding different model updating strategies.*

### What AI Is Used For

- **Beguš, Nina.** “Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling.” *Nature*, 28 Oct. 2024. [nature.com/articles/s41599-024-03868-8](https://www.nature.com/articles/s41599-024-03868-8)  
  *Compares storytelling from GPT models and human authors under identical prompts. Provides a benchmark for evaluating generated stories.*

- **Hicke, Rebecca M. M., et al.** “Says Who? Effective Zero-Shot Annotation of Focalization.” *arXiv.Org*, 17 Sept. 2024. [arxiv.org/abs/2409.11390](https://arxiv.org/abs/2409.11390)  
  *Demonstrates using LLMs to annotate literary texts for focalization. Relevant for literary and text analysis of generated stories.*

- **Sakellaridis, Pavlos.** “Exploring the Potential of LLM-Based Agents as Dungeon Masters.” *Studednttheses.Uu.Nl*, July 2024. [studenttheses.uu.nl/handle/20.500.12932/47209](https://studenttheses.uu.nl/bitstream/handle/20.500.12932/47209/Thesis_Final.pdf?sequence=1&isAllowed=y)  
  *Examines the use of LLMs as Dungeon Masters in D&D, trained on Critical Role datasets. Provides inspiration for interactive narrative applications.*

- **Walsh, Melanie, Anna Preus, and Maria Antoniak.** “Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets.” *arXiv.Org*, 10 Oct. 2024. [arxiv.org/abs/2406.18906](https://arxiv.org/abs/2406.18906)  
  *Explores benchmarking LLM-generated poetry against human forms. Useful model for benchmarking AI-generated fairy tales in the case study.*

### General Resources

- **The AI for Humanists Project.** *AI for Humanists.* [aiforhumanists.com](https://aiforhumanists.com)  
  *A collection of tutorials and resources designed to introduce humanities scholars to text classification, word embeddings, and LLM applications. Includes a notebook on zero-shot prompting and an extensive glossary of key AI terms. Useful as a reference throughout this study for concepts, methods, and practical techniques. Can also serve as a great next step for your own AI journey.*


- **Willson, Simon.** “Things We Learned about LLMs in 2024.” *simonwillison.net*, 31 Dec. 2024. [simonwillison.net/2024/Dec/31/llms-in-2024](https://simonwillison.net/2024/Dec/31/llms-in-2024/#some-of-those-gpt-4-models-run-on-my-laptop)  
  *A collection of blog posts on emerging LLM trends and practical applications. Offers technical insights and project inspiration.*

